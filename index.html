<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="We introduce the first autonomous LLM agent which we call "reason for future, act for now" with provable regret guarantees and outstanding empirical performances.">
  <meta name="keywords" content="Large Language Model, Agent">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Reason for Future, Act for Now: A Principled Framework for Autonomous LLM Agents with Provable Sample Efficiency</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Reason for Future, Act for Now: A Principled Framework for Autonomous LLM Agents with Provable Sample Efficiency</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=uEl_TtkAAAAJ&hl=en">Zhihan Liu</a><sup>*1</sup>,</span>
            <span class="author-block">
              <a href="http://mousehu.cn/">Hao Hu</a><sup>*2</sup>,</span>
            <span class="author-block">
              <a href="https://shenao-zhang.github.io/">Shenao Zhang</a><sup>*1</sup>,</span>
            </span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=bzPCv_8AAAAJ&hl=en">Hongyi Guo</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://openreview.net/profile?id=~Shuqi_Ke1">Shuqi Ke</a><sup>3</sup>,
            </span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=1G8RH_YAAAAJ&hl=en">Boyi Liu</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://zhaoranwang.github.io/">Zhaoran Wang</a><sup>1</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Northwestern University,</span>
            <span class="author-block"><sup>2</sup>Tsinghua University,</span>
            <span class="author-block"><sup>3</sup>The Chinese University of Hong Kong</span><br>
			      <small>(* indicate equal contribution)</small><br>
          </div>
          

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2309.17382.pdf"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/2309.17382"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/agentification/RAFA_code"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                </a>
              </span>
              <!-- Dataset Link. -->
              <!-- <span class="link-block">
                <a href="https://github.com/askforalfred/alfred"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                </a> -->
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <h2 class="subtitle has-text-centered">
        <b>Overview:</b> We introduce the first autonomous LLM agent which we call "reason for future, act for now" (<span class="dnerf">RAFA</span>) with provable regret guarantees and outstanding empirical performances.
      </h2>
      <img src="./static/images/intro.svg" width="100%"/>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>Large language models (LLMs) demonstrate impressive reasoning abilities, but translating reasoning into actions in the real world remains challenging. In particular, it remains unclear how to complete a given task provably within a minimum number of interactions with the external environment, e.g., through an internal mechanism of reasoning. To this end, we propose a principled framework with provable regret guarantees to orchestrate reasoning and acting, which we call "reason for future, act for now" (<span class="dnerf">RAFA</span>). Specifically, we design a prompt template for reasoning that learns from the memory buffer and plans a future trajectory over a long horizon ("reason for future"). At each step, the LLM agent takes the initial action of the planned trajectory ("act for now"), stores the collected feedback in the memory buffer, and reinvokes the reasoning routine to replan the future trajectory from the new state.
          </p>
          <p>The key idea is to cast reasoning in LLMs as learning and planning in Bayesian adaptive Markov decision processes (MDPs). Correspondingly, we prompt LLMs to form an updated posterior of the unknown environment from the memory buffer (learning) and generate an optimal trajectory for multiple future steps that maximizes a value function (planning). The learning and planning subroutines are performed in an "in-context" manner to emulate the actor-critic update for MDPs. Our theoretical analysis proves that the novel combination of long-term reasoning and short-term acting achieves a √T regret. In particular, the regret bound highlights an intriguing interplay between the prior knowledge obtained through pretraining and the uncertainty reduction achieved by reasoning and acting. Our empirical validation shows that it outperforms various existing frameworks and achieves nearly perfect scores on a few benchmarks. By incorporating “classical” MDP techniques, RAFA introduces the first autonomous LLM agent with provable regret guarantees. Notably, LLMs do not function as actors, critics, or learned world models, but rather as an internal mechanism that improves them iteratively.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
</section>


<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        
        <h2 class="title is-3">Method</h2>
        <img src="./static/images/algo1.jpg"/>
        <div class="content has-text-justified">
          <p>At the t-th step of Algorithm 1, the LLM agent invokes the reasoning routine, which learns from the memory buffer and plans a future trajectory over a long horizon ("reason for future" in Line 6), takes the initial action of the planned trajectory (“act for now” in Line 7), and stores the collected feedback (state, action, and reward) in the memory buffer (Line 8). Upon the state transition of the external environment, the LLM agent reinvokes the reasoning routine to replan another future trajectory from the new state (Line 6 following Line 9). To ensure the learning and planning stability, we impose the switching condition (Line 10) to decide whether to incorporate the newest chunk of history into the information state, which is used in the reasoning routine as contexts.   
          </p>
        </div>		
        <h2 class="title is-3">Experiments</h2>
        <h2 class="title is-4">(a) Game of 24</h2>
  	<p>RAFA uses the ToT planner in Game of 24.</p>
		<figure>
    		<img src="./static/images/result_24.jpg" width="100%"/>
		<figcaption style="text-align: center; font-style: italic; color: #888;">
		    Sample efficiency on Game of 24.
  		</figcaption>
		</figure>
        <h2 class="title is-4">(b) ALFWorld</h2>
  	<p>RAFA uses the BFS planner in ALFWorld.</p>
	      	<figure>
    		<img src="./static/images/combine_alf.svg" width="100%"/>
		<figcaption style="text-align: center; font-style: italic; color: #888;">
		    Results on ALFWorld.
  		</figcaption>
		</figure>
	<video id="teaser" autoplay muted loop playsinline height="110%">
        <source src="./static/images/demo_alf.mp4"
                type="video/mp4">
      	</video>
        <h2 class="title is-4">(c) BlocksWorld</h2>
  	<p>RAFA uses the MCTS planner in BlocksWorld.</p>
	      	<figure>
    		<img src="./static/images/result_blocks1.jpg" width="100%"/>
		<img src="./static/images/result_blocks2.jpg" width="100%"/>
		<figcaption style="text-align: center; font-style: italic; color: #888;">
		    Sample efficiency on BlocksWorld.
  		</figcaption>
		</figure>

        <h2 class="title is-4">(d) Tic-Tac-Toe</h2>
  	<p>RAFA uses the MCTS planner in Tic-Tac-Toe.</p>
	      	<figure>
    		<img src="./static/images/combine_ttt.svg" width="100%"/>
		<figcaption style="text-align: center; font-style: italic; color: #888;">
		    Results on Tic-Tac-Toe.
  		</figcaption>
		</figure>
		</figure>
      </div>
    </div>
  </div>
</section>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@misc{liu2023reason,
      title={Reason for Future, Act for Now: A Principled Framework for Autonomous LLM Agents with Provable Sample Efficiency}, 
      author={Zhihan Liu and Hao Hu and Shenao Zhang and Hongyi Guo and Shuqi Ke and Boyi Liu and Zhaoran Wang},
      year={2023},
      eprint={2309.17382},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="content has-text-justified">
        <p>
         We acknowledge <a
            href="https://github.com/nerfies/nerfies.github.io">Nerfies</a> for the website template.
        </p>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
